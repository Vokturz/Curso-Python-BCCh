{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzQ2xK0k7IOvuFaAFo6p37",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vokturz/Curso-Python-BCCh/blob/main/clase4/Clase4_Pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pandas\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/Vokturz/Curso-Python-BCCh/blob/main/clase4/pandas_dalle3.png?raw=true\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "La librería Pandas se utiliza para el análisis y manipulación de datos. Se basa en estructuras de datos eficientes y proporciona herramientas esenciales para trabajar con datos tabulares (similares a las tablas de bases de datos o hojas de cálculo).\n",
        "\n",
        "Para inicializarla, basta con importarla dentro de python:\n",
        "```python\n",
        "import pandas as pd\n",
        "```\n",
        "\n",
        "## Estructuras de Datos\n",
        "\n",
        "Existen dos estructuras de datos principales en Pandas, ambas basadas en los `arrays` de Python: Series y DataFrame.\n",
        "\n",
        "### Series\n",
        "\n",
        "Una Serie es similar a un arreglo unidimensional, pero con etiquetas. Esto último permite acceder a los elementos de una forma muy similar a los diccionarios, sin embargo, a diferencia de estos, una Serie **si** permite elementos duplicados."
      ],
      "metadata": {
        "id": "eCZ7_nTdAtM1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dN3Jti_APGS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Generamos una serie\n",
        "serie = pd.Series([1, 2, 3, 4], index=[\"a\", \"b\", \"c\", \"d\"])\n",
        "print(serie)\n",
        "\n",
        "# Podemos mirar cuales son los indices\n",
        "print(serie.index)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos generar la serie a partir de un diccionario\n",
        "un_dict = {\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4}\n",
        "serie = pd.Series(un_dict)\n",
        "print(serie)\n",
        "print(\"---\")\n",
        "\n",
        "# Podemos acceder a un elemento de forma muy similar a un diccionario\n",
        "print(\"El valor de a es\", serie[\"a\"])\n",
        "\n",
        "\n",
        "serie[\"a\"] = 0 # Podemos modificar un valor\n",
        "print(\"El valor de a es\", serie[\"a\"])\n",
        "type(serie[\"a\"])"
      ],
      "metadata": {
        "id": "RNVNkeQnBpUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Una serie con indices duplicados\n",
        "serie = pd.Series([1, 2, 3, 4], index=[\"a\", \"b\", \"c\", \"a\"])\n",
        "print(serie)\n",
        "print(\"---\")\n",
        "\n",
        "# En este caso, al pedir el valor de \"a\" nos mostrará dos valores\n",
        "print(serie[\"a\"])\n",
        "type(serie[\"a\"])"
      ],
      "metadata": {
        "id": "0vvaJQTIDTzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataFrame\n",
        "Un DataFrame es como un diccionario de Series. Podemos entenderlo como una tabla de datos donde las columnas son Series que comparten un índice común.\n"
      ],
      "metadata": {
        "id": "e-rVkMv-FbD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    'columna1': [1, 2, 3, 4],\n",
        "    'columna2': ['a', 'b', 'c', 'd']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "print('---')\n",
        "\n",
        "# Podemos asignar indices\n",
        "df = pd.DataFrame(data, index=[\"id1\", \"id2\", \"id3\", \"id4\"])\n",
        "print(df)"
      ],
      "metadata": {
        "id": "5DWTMy9TBuTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Si omitimos el print, en algunas plataformas los DataFrames se muestran como una tabla\n",
        "df"
      ],
      "metadata": {
        "id": "ZBud98xkByo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos acceder a un elemento\n",
        "columna2 = df[\"columna2\"]\n",
        "print(columna2)\n",
        "type(columna2) # Vemos que es una Serie"
      ],
      "metadata": {
        "id": "hhkiKvRYHoDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos agregar una nueva columna\n",
        "df[\"columna3\"] = [True, True, False ,False]\n",
        "df"
      ],
      "metadata": {
        "id": "O4DpjTS2C6Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos asignar una columna como el nuevo indice\n",
        "df.set_index(\"columna1\")  # Esto retorna el DataFrame modificado\n",
        "\n",
        "# Si hubieramos querido guardarlo, deberíamos haber usado:\n",
        "# df = df.set_index(\"columna1\")\n",
        "# df.set_index(\"columna1\", inplace=True) # si inplace=True no retorna nada"
      ],
      "metadata": {
        "id": "RuBgx7BxHV6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos acceder a las columnas\n",
        "print(df.columns)\n",
        "\n",
        "# Y tambien podemos modificar sus nombres\n",
        "df.columns = [\"col1\", \"col2\", \"col3\"]\n",
        "df"
      ],
      "metadata": {
        "id": "BcOmnfXmfFBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# renombrar un indice (no inplace)\n",
        "df.rename({'id1' : 'indice1'})"
      ],
      "metadata": {
        "id": "ZSX9-Ii6fa_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# renombrar una columna (no inplace)\n",
        "df.rename(columns={'col1' : 'columna1'})"
      ],
      "metadata": {
        "id": "6tUol8cUfnX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como mencionamos, los DataFrames (así como las Series) están basados en arrays de NumPy, por lo que podemos usar las mismas operaciones que haríamos sobre ellos."
      ],
      "metadata": {
        "id": "DwKCK1e3JHKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos ver que los valores están representados por un array\n",
        "df.values"
      ],
      "metadata": {
        "id": "Bg6kKrAHI7Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A col1 le sumamos 10\n",
        "df[\"col1\"] + 10"
      ],
      "metadata": {
        "id": "QJFb--YuIxuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una propiedad interesante de los arrays es que podemos filtrar según algun valor. Por ejemplo si mi array contiene muchos numeros, podemos filtrar para obtener todos los valores que son mayor a un número:\n",
        "```python\n",
        "# valores de un_array que son mayores a N\n",
        "un_array[un_array > N]\n",
        "```\n",
        "\n",
        "Esto mismo se puede utilizar en Pandas!"
      ],
      "metadata": {
        "id": "ueXsK-dpJt5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo con NumPy\n",
        "import numpy as np\n",
        "un_array = np.arange(1,21)\n",
        "print(un_array)\n",
        "print(\"---\")\n",
        "print(un_array[un_array > 10])"
      ],
      "metadata": {
        "id": "u2MhW3CNJYqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo con Pandas, las siguientes tres expresiones hacen lo mismo\n",
        "# Para las últimas dos, es importante que la columna no tenga espacios\n",
        "df[df[\"col1\"] > 2]\n",
        "# df[df.col1 > 2]\n",
        "# df.query(\"col1 > 2\")\n"
      ],
      "metadata": {
        "id": "_T1gC71iJrVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Volvamos a los indicadores mensuales\n",
        "indicadores_mensuales = {\n",
        "      \"IPC\": [0.8, -0.1, 1.1, 0.3, 0.1, -0.2],\n",
        "      \"Tasa de Desempleo\": [8.04, 8.37, 8.81, 8.66, 8.52, 8.53],\n",
        "      \"Imacec\": [0.2, 5, -2.1, -0.9, -0.8, -0.2]\n",
        "  }\n",
        "\n",
        "indice_meses = {0: \"Enero\",\n",
        "                1: \"Febrero\",\n",
        "                2: \"Marzo\",\n",
        "                3: \"Abril\",\n",
        "                4: \"Mayo\",\n",
        "                5: \"Junio\"}\n",
        "\n",
        "df_indicadores = pd.DataFrame(indicadores_mensuales, index=indice_meses.values())\n",
        "df_indicadores"
      ],
      "metadata": {
        "id": "vVef5zWqRs3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos trasponer los datos, de esta forma las filas pasan a ser\n",
        "# columnas y viceversa\n",
        "df_indicadores.T"
      ],
      "metadata": {
        "id": "Z5cd3u9HF47b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Promedio sobre cada columna, redondeado a 2\n",
        "df_indicadores.mean().round(2)"
      ],
      "metadata": {
        "id": "D6gI7CV_SGMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exisiste un método que nos da un análisis estadístico\n",
        "# descriptivo de forma inmediata\n",
        "df_indicadores.describe().round(2)"
      ],
      "metadata": {
        "id": "mwsdz2QcSLsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_indicadores.idxmin() # Indice mínimo"
      ],
      "metadata": {
        "id": "Ax9fT3GNSeFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge\n",
        "La operación [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) permite unir dos conjuntos de datos a partir de columnas (o el indice) en común. La expresión es la siguiente:\n",
        "\n",
        "```python\n",
        "pd.merge(df1, df2, on=[\"col1\"], how=\"left\")\n",
        "```\n",
        "donde `how` corresponde al tipo de unión que se quiere realizar:\n",
        "- `left`: Mantiene las filas del DataFrame izquierdo y agrega columnas del DataFrame derecho. Es análogo al `left join` en SQL.\n",
        "- `right`: Similar al anterior, pero sobre las filas del DataFrame derecho.\n",
        "- `inner`: Mantiene las filas con claves coincidentes en ambos DataFrames, es decir, las filas que sin coincidencia quedan excluidas. Es análogo al `ìnner join` en SQL.\n",
        "- `outer`: Incluye las files de ambos DataFrames. Si hay claves no coincidentes, estas quedan con valores nulos (NaN). Análogo al `full join` de SQL\n",
        "\n",
        "<div>\n",
        "<img src=\"https://www.csestack.org/wp-content/uploads/2020/10/sql-table-joins.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "> Fuente: *https://www.csestack.org/sql-join/*"
      ],
      "metadata": {
        "id": "GmRIrW4GZjW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Datos de tasas de interés\n",
        "tasas = pd.DataFrame({\n",
        "    'Año': [2020, 2021, 2022],\n",
        "    'Tasa de Interés (%)': [2.5, 2.0, 2.25]\n",
        "})\n",
        "\n",
        "# Datos de inflación\n",
        "inflacion = pd.DataFrame({\n",
        "    'Año': [2021, 2022, 2023],\n",
        "    'Inflación (%)': [3.1, 3.5, 2.8]\n",
        "})\n",
        "\n",
        "# Merge de los dos dataframes usando 'Año' como clave\n",
        "df_merged = pd.merge(tasas, inflacion, on='Año', how='left')\n",
        "df_merged"
      ],
      "metadata": {
        "id": "TD7TgKeLZZYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pivoteo\n",
        "La operación `pivot` en Pandas nos permite reestructurar un DataFrame, de forma que los elementos dos columnas pasan a ser los indices y columnas de un nuevo DataFrame. La sintaxis es la siguiente:\n",
        "\n",
        "```python\n",
        "df.pivot(index=\"col1\", columns=\"col2\", values=\"col3\")\n",
        "```\n",
        "\n",
        "Donde:\n",
        "- `index`: La columna que se utilizará como índice del nuevo DataFrame.\n",
        "- `columns`: La columna que se utilizará para crear las nuevas columnas.\n",
        "- `values`: La columna que contiene los valores que se quieren distribuir en las nuevas columnas."
      ],
      "metadata": {
        "id": "2Wo5s9VpdDel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Datos de inflación mensual\n",
        "inflacion_mensual = pd.DataFrame({\n",
        "    'Año': [2021, 2021, 2021, 2022, 2022, 2022],\n",
        "    'Mes': ['Enero', 'Febrero', 'Marzo', 'Enero', 'Febrero', 'Marzo'],\n",
        "    'Inflación (%)': [0.3, 0.4, 0.35, 0.32, 0.38, 0.33]\n",
        "})\n",
        "inflacion_mensual"
      ],
      "metadata": {
        "id": "hppCqkozeV4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pivoteamos\n",
        "df_pivoteado = inflacion_mensual.pivot(index='Año', columns='Mes', values='Inflación (%)')\n",
        "df_pivoteado"
      ],
      "metadata": {
        "id": "XQlNCdkVebDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargando archivos\n",
        "Pandas permite cargar archivos de distintas fuentes, basta con usar la función de Pandas correcta según el tipo de archivo:\n",
        "\n",
        "```python\n",
        "# Cargar desde un archivo CSV\n",
        "df = pd.read_csv('ruta/del/archivo.csv')\n",
        "\n",
        "# Cargar desde un archivo Excel\n",
        "# NOTA: Es importante que el Excel tenga un buen formato\n",
        "df = pd.read_excel('ruta/del/archivo.xlsx')\n",
        "\n",
        "# Cargar desde un archivo Stata\n",
        "df = pd.read_excel('ruta/del/archivo.dta')\n",
        "\n",
        "# Cargar desde un archivo SQL\n",
        "df = pd.read_excel('ruta/del/archivo.sql')\n",
        "```\n",
        "\n",
        "En este caso `ruta/del/archivo` corresponde a la ubicación del archivo dentro de nuestro computador. No obstante, podemos también cargar un archivo desde un link:\n",
        "\n",
        "```python\n",
        "# Cargar desde un archivo CSV en internet\n",
        "df = pd.read_csv('https://url_al_archivo_csv')\n",
        "```\n",
        "\n",
        "### Ejemplo práctico\n",
        "\n",
        "A continuación se mostrará un ejemplo de cómo leer un archivo y sacar ciertas estadísticas sobre él. Para ello, normalmente se realizan ciertos pasos:\n",
        "\n",
        "1. **Carga del archivo**: Como se mencionó anteriormente, ya sea desde una ubicación local o desde un enlace en línea.\n",
        "2. **Exploración inicial**: Antes de cualquier análisis, es crucial familiarizarse con la naturaleza y estructura de los datos.\n",
        "3. **Limpieza de datos**: Este paso implica tratar los valores faltantes, remover duplicados, corregir errores, entre otras acciones necesarias para garantizar la calidad de los datos.\n",
        "4. **Análisis y obtención de estadísticas**: Una vez que los datos están limpios, podemos proceder a realizar diferentes análisis y obtener estadísticas relevantes."
      ],
      "metadata": {
        "id": "dgrHgWUvLcaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Datos de población en Chile por Región y Comuna\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/input/DistribucionDEIS/baseFiles/DEIS_template.csv')\n",
        "print(df.shape) # Mostrar las dimensiones\n",
        "# 2. Mostrar las primeras (5) filas\n",
        "df.head()"
      ],
      "metadata": {
        "id": "aU2FnoFZMKuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Info general de la tabla\n",
        "# object es cualquier tipo no numerico (lista, tupla, strings, etc)\n",
        "# en este caso corresponden a strings\n",
        "df.info()"
      ],
      "metadata": {
        "id": "T9JaddzYCY37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Veamos la cantidad de valores NaN\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "EpqqQYSiVKZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. En particular, nos interesan los NaN en la columna poblacion\n",
        "df[df['Poblacion'].isna()]"
      ],
      "metadata": {
        "id": "snUnKc0dVK7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Eliminamos los NaN de Población\n",
        "df = df.dropna(subset=[\"Poblacion\"])\n",
        "print(df.shape) # Deberían haber 346 comunas\n",
        "df[\"Poblacion\"].sum() # ?? No parece ser correcto"
      ],
      "metadata": {
        "id": "zVbXfiDpVPSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Revisamos las primeras 10 filas\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "aZE2d9prW-6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Descartamos los valores donde Comuna == Total\n",
        "df = df.query(\"Comuna != 'Total'\")\n",
        "print(df.shape) # Ahora si hay 346 comunas!\n",
        "df[\"Poblacion\"].sum() # Parece andar bien"
      ],
      "metadata": {
        "id": "NbJURR_jXK-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Vemos estadísticas en Población\n",
        "df[\"Poblacion\"].describe()"
      ],
      "metadata": {
        "id": "7wdiE5q_CVCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos guardar el  resultado\n",
        "df.to_csv(\"poblacion_chile2020.csv\", index=False) # no nos interesan los indices"
      ],
      "metadata": {
        "id": "hIcp-gx90_bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GroupBy\n",
        "\n",
        "La operación `groupby` en Pandas permite agrupar el DataFrame usando una columna específica (o más) y luego aplicar una función de agregación (como suma, media, conteo, etc.) a cada grupo.\n",
        "\n",
        "Por ejemplo, para los datos de población a nivel de comuna, podemos agrupar por la región para obtener estadísticas como el total de población, la cantidad total de comunas, entre otros."
      ],
      "metadata": {
        "id": "QA18sheUC3iX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para cada región, contar cuantas comunas hay\n",
        "df.groupby(\"Region\")[\"Comuna\"].count()"
      ],
      "metadata": {
        "id": "YGwqJnLLCsNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para cada región, obtener la población total y ordenarlo\n",
        "df.groupby(\"Region\")[\"Poblacion\"].sum().sort_values()"
      ],
      "metadata": {
        "id": "hNdICcpZDsT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos obtener la estadística descriptiva de la población a nivel de Región\n",
        "df.groupby(\"Region\")['Poblacion'].describe()"
      ],
      "metadata": {
        "id": "NtIIpQRmD2bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una forma eficiente de agregar valores, es usar el operador `.agg`:\n",
        "\n",
        "```python\n",
        "df.groupby([\"col1\", \"col2\"]).agg(promedio_col3=(\"col3\", \"mean\"),\n",
        "                                 total=(\"col4\", \"count\"))\n",
        "```\n",
        "\n",
        "Al usar este operador, debemos crear elementos separados por coma, donde cada columna está descrita por una tupla. Así por ejemplo, en este caso definimos una columna `promedio_col3` como el promedio (`mean`) de la columna `col3`."
      ],
      "metadata": {
        "id": "fhCpmghHEeyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agregamos a nivel de región, y obtenemos la población total junto\n",
        "# a la cantidad total de comunas\n",
        "df.groupby(\"Region\").agg(total_poblacion=(\"Poblacion\", \"sum\"),\n",
        "                         total_comunas=(\"Comuna\", \"count\"))"
      ],
      "metadata": {
        "id": "D3NlBqQGEHtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Si revisamos la columna Region, vemos que los elementos se repiten bastante\n",
        "df['Region']"
      ],
      "metadata": {
        "id": "Vi_FQUCyE_PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos obtener los elementos únicos de una columna\n",
        "print(df[\"Region\"].unique())\n",
        "# Tambien podemos contar cuantas elementos únicos hay\n",
        "print(\"Total regiones:\", df[\"Region\"].nunique())"
      ],
      "metadata": {
        "id": "DBSuqhhfG76A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Una operación interesante es la de value_counts()\n",
        "df[\"Region\"].value_counts()"
      ],
      "metadata": {
        "id": "I08GvM_GJp4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos obtener una correlación entre la poblaciony la cantidad de columnas\n",
        "resultado = df.groupby(\"Region\").agg(total_poblacion=(\"Poblacion\", \"sum\"),\n",
        "                                     total_comunas=(\"Comuna\", \"count\"))\n",
        "resultado.corr(method=\"spearman\") # también puede ser pearson o kendall"
      ],
      "metadata": {
        "id": "d59pjjN8J9dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5cd9d17-9d18-4032-92d0-227a25958789_2985x2823.jpeg\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "> Fuente: *https://www.blog.dailydoseofds.com/p/the-biggest-limitation-of-pearson*\n"
      ],
      "metadata": {
        "id": "GI0bdtLeLlNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Un ejemplo de lo que se viene para la siguiente clase\n",
        "resultado.plot.scatter(\"total_poblacion\", \"total_comunas\")"
      ],
      "metadata": {
        "id": "qAJOPizHKwh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Links de interés\n",
        "- [Pandas](https://pandas.pydata.org/docs/user_guide/10min.html) tiene una guía introductoria de 10 minutos\n",
        "- [W3Schools](https://www.w3schools.com/python/pandas/default.asp) tiene un tutorial de Pandas, incluyendo [algunos ejercicios](https://www.w3schools.com/python/pandas/exercise.asp)\n",
        "- El [Instituto Nacional de Estadísticas](https://www.ine.gob.cl/estadisticas) almacena datos de distinta índole\n",
        "- El Gobierno tiene un [portal de datos](https://datos.gob.cl/en/dataset)\n",
        "- [Kaggle](https://www.kaggle.com/datasets) posee más de 200.000 set de datos de tópicos muy variados"
      ],
      "metadata": {
        "id": "acPOxnx8GWxa"
      }
    }
  ]
}